{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "CUDA Version Used by PyTorch: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version Used by PyTorch:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "CUDA Version Used by PyTorch: 12.1\n",
      "GPU Name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Version Used by PyTorch:\", torch.version.cuda)\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your setup.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from torchvision import models\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import Subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in the system: 20\n"
     ]
    }
   ],
   "source": [
    "cpuCount = os.cpu_count()\n",
    "print(\"Number of CPUs in the system:\", cpuCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Dataset 1 (WM811K)\n",
    "def preprocess_wm811k_data(path_to_dataset, target_size=(32, 32), num_classes =38):\n",
    "    \"\"\"\n",
    "    Load and preprocess the WM811K dataset.\n",
    "\n",
    "    Parameters:\n",
    "        path_to_dataset (str): Path to the WM811K .pkl dataset.\n",
    "        target_size (tuple): Desired image size.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed wafer maps.\n",
    "        np.ndarray: Labels.\n",
    "    \"\"\"\n",
    "    df = pd.read_pickle(path_to_dataset)\n",
    "    df = df.drop(['waferIndex'], axis=1)\n",
    "    df['waferMapDim'] = df['waferMap'].apply(lambda x: (np.size(x, axis=0), np.size(x, axis=1)))\n",
    "    df['failureNum'] = df['failureType']\n",
    "\n",
    "    mapping_type = {\n",
    "        'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3,\n",
    "        'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8\n",
    "    }\n",
    "    df = df.replace({'failureNum': mapping_type})\n",
    "    df_withpattern = df[(df['failureNum'] >= 0)].reset_index()\n",
    "\n",
    "    wafer_maps = df_withpattern['waferMap'].to_numpy()\n",
    "    labels = df_withpattern['failureNum'].to_numpy()\n",
    "\n",
    "    processed_maps = []\n",
    "    for wafer_map in wafer_maps:\n",
    "        wafer_map = np.array(wafer_map, dtype=np.float32)\n",
    "        wafer_map = (wafer_map - np.min(wafer_map)) / (np.max(wafer_map) - np.min(wafer_map))\n",
    "        scaling_factor = min(target_size[1] / wafer_map.shape[1], target_size[0] / wafer_map.shape[0])\n",
    "        new_width = int(wafer_map.shape[1] * scaling_factor)\n",
    "        new_height = int(wafer_map.shape[0] * scaling_factor)\n",
    "        resized_map = cv2.resize(wafer_map, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "        padded_map = np.full(target_size, 0, dtype=np.float32)\n",
    "        x_offset = (target_size[1] - new_width) // 2\n",
    "        y_offset = (target_size[0] - new_height) // 2\n",
    "        padded_map[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_map\n",
    "        processed_maps.append(padded_map)\n",
    "\n",
    "    # Oversampling for balanced dataset\n",
    "    combined = [(x, y) for x, y in zip(processed_maps, labels)]\n",
    "    class_samples = {cls: [sample for sample in combined if sample[1] == cls] for cls in np.unique(labels)}\n",
    "    max_class_size = max(len(samples) for samples in class_samples.values())\n",
    "    oversampled_samples = []\n",
    "    for cls, samples in class_samples.items():\n",
    "        oversampled_samples.extend(resample(samples, replace=True, n_samples=max_class_size, random_state=42))\n",
    "    np.random.shuffle(oversampled_samples)\n",
    "\n",
    "    data = np.array([sample[0] for sample in oversampled_samples])\n",
    "    labels = np.array([sample[1] for sample in oversampled_samples])\n",
    "    data = np.expand_dims(data, axis=1)\n",
    "\n",
    "    # One-hot encode labels\n",
    "    encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "    labels = labels.reshape(-1, 1)  # Reshape for one-hot encoding\n",
    "    one_hot_labels = encoder.fit_transform(labels)\n",
    "\n",
    "    # Pad one-hot labels to 38 dimensions\n",
    "    padded_labels = np.zeros((len(one_hot_labels), num_classes))\n",
    "    padded_labels[:, :one_hot_labels.shape[1]] = one_hot_labels\n",
    "\n",
    "    return data, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Dataset 2 (MixedWM38)\n",
    "def preprocess_npz_dataset(path, target_size=(32, 32), num_classes=38):\n",
    "    \"\"\"\n",
    "    Preprocess the .npz dataset for mixed-type wafer maps.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the .npz dataset.\n",
    "        target_size (tuple): Desired image size.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed wafer maps.\n",
    "        np.ndarray: Multi-label one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    data = np.load(path)\n",
    "    wafer_maps = data['arr_0']\n",
    "    labels = data['arr_1']\n",
    "\n",
    "    # Pad labels to match the model's expected number of classes\n",
    "    padded_labels = np.zeros((labels.shape[0], num_classes))\n",
    "    padded_labels[:, :labels.shape[1]] = labels\n",
    "\n",
    "    processed_maps = []\n",
    "    for wafer_map in wafer_maps:\n",
    "        wafer_map = np.array(wafer_map, dtype=np.float32)\n",
    "        wafer_map = (wafer_map - np.min(wafer_map)) / (np.max(wafer_map) - np.min(wafer_map))\n",
    "        resized_map = cv2.resize(wafer_map, target_size, interpolation=cv2.INTER_AREA)\n",
    "        processed_maps.append(resized_map)\n",
    "\n",
    "    processed_maps = np.expand_dims(processed_maps, axis=1)\n",
    "    return np.array(processed_maps), padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for PyTorch\n",
    "class WaferDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Classifier(nn.Module):\n",
    "    def __init__(self, init_weights=True, num_classes=38, feature_dim=512):\n",
    "        \"\"\"\n",
    "        VGG16-based Classifier for multi-label classification.\n",
    "\n",
    "        Parameters:\n",
    "            init_weights (bool): Whether to initialize weights.\n",
    "            num_class (int): Number of output classes.\n",
    "            feature_dim (int): Feature dimensionality from the feature extractor.\n",
    "        \"\"\"\n",
    "        super(VGG16Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            #nn.Dropout(),\n",
    "            nn.Linear(feature_dim, 4096),\n",
    "            #nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            #nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten features for linear layers\n",
    "        x = self.classifier(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid for multi-label classification\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the classifier layers.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDADA:\n",
    "    def __init__(self, train_data, train_labels,test_data,test_labels,*, num_classes=38, max_epoch=100, batch_size=32, learning_rate=0.0001, subset_size=None):\n",
    "        \"\"\"\n",
    "        Initializes the CDADA training and evaluation pipeline.\n",
    "\n",
    "        Parameters:\n",
    "            train_data (np.ndarray): Training data.\n",
    "            train_labels (np.ndarray): Labels for the training data.\n",
    "            test_data (np.ndarray): Testing data.\n",
    "            test_labels (np.ndarray): Labels for the testing data.\n",
    "            num_classes (int): Number of classes for multi-label classification.\n",
    "            max_epoch (int): Maximum number of epochs.\n",
    "            batch_size (int): Batch size for training and testing.\n",
    "            learning_rate (float): Learning rate for optimizers.\n",
    "        \"\"\"\n",
    "        # Create subset if subset_size is specified\n",
    "        if subset_size is not None:\n",
    "                    # Generate random shuffled indices for the dataset\n",
    "            total_indices = np.arange(len(train_data))\n",
    "            np.random.shuffle(total_indices)\n",
    "            \n",
    "            # Select the first subset_size indices\n",
    "            subset_indices = total_indices[:subset_size]\n",
    "            \n",
    "            # Create the subset dataset using the shuffled indices\n",
    "            subset_data = train_data[subset_indices]\n",
    "            subset_labels = train_labels[subset_indices]\n",
    "            self.train_loader = DataLoader(WaferDataset(subset_data, subset_labels), batch_size=batch_size, shuffle=True)\n",
    "        else:\n",
    "            # Use full dataset\n",
    "            self.train_loader = DataLoader(WaferDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
    "        #self.train_loader = DataLoader(WaferDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(WaferDataset(test_data, test_labels), batch_size=batch_size, shuffle=False)\n",
    "        #self.pkl_file_path = pkl_file_path\n",
    "        #self.npz_file_path = npz_file_path\n",
    "        self.num_classes = num_classes\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "\n",
    "        # Initialize VGG16 model\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  # Modify for grayscale input\n",
    "        self.feature_extractor = model.features.to('cuda')\n",
    "        self.classifier = VGG16Classifier(num_classes=num_classes).to('cuda')\n",
    "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # Initialize classifiers\n",
    "        #feature_dim = 512\n",
    "    \n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.opt_generator.zero_grad()\n",
    "        self.opt_classifier.zero_grad()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model on the source dataset.\n",
    "        \"\"\"\n",
    "        self.feature_extractor.train()\n",
    "        self.classifier.train()\n",
    "       \n",
    "\n",
    "        for ep in range(self.max_epoch):\n",
    "            running_loss = 0\n",
    "            for images, labels in self.train_loader:\n",
    "                images, labels = images.to('cuda'), labels.to('cuda')\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                features = self.feature_extractor(images)\n",
    "                outputs = self.classifier(features)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "        \n",
    "            print(f\"Epoch {ep + 1}/{self.max_epoch}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "            \n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Tests the model on the MixedWM38 dataset.\n",
    "        \"\"\"\n",
    "        self.feature_extractor.eval()\n",
    "        self.classifier.eval()\n",
    "\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.test_loader:\n",
    "                images, labels = images.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        \n",
    "\n",
    "                features = self.feature_extractor(images)\n",
    "                outputs = self.classifier(features)\n",
    "                all_labels.append(labels.cpu())\n",
    "                all_outputs.append(outputs.cpu())\n",
    "                # Accuracy calculation\n",
    "                predicted = (outputs > 0.5).float()  # Apply threshold for multi-label classification\n",
    "                correct_predictions += (predicted == labels).all(dim=1).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_outputs = torch.cat(all_outputs)\n",
    "        # Calculate accuracy\n",
    "        accuracy = 100 * correct_predictions / total_samples\n",
    "        precision = precision_score(all_labels, all_outputs.round(), average='samples')\n",
    "        recall = recall_score(all_labels, all_outputs.round(), average='samples')\n",
    "        f1 = f1_score(all_labels, all_outputs.round(), average='samples')\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 109.3564\n",
      "Epoch 2/50, Loss: 81.9239\n",
      "Epoch 3/50, Loss: 73.7396\n",
      "Epoch 4/50, Loss: 68.7768\n",
      "Epoch 5/50, Loss: 64.8640\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data, train_labels = preprocess_wm811k_data(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\")\n",
    "test_data, test_labels = preprocess_npz_dataset(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/Wafer_Map_Datasets.npz\", target_size=(32, 32))\n",
    "cdada = CDADA(train_data, train_labels, test_data, test_labels, num_classes=38, max_epoch=50, subset_size = 80000)\n",
    "cdada.train()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdada.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def visualize_dataset(data, labels, title=\"Dataset Visualization\", classes=None, cmap='gray'):\n",
    "    \"\"\"\n",
    "    Visualizes the class distribution and random samples from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (np.ndarray): Wafer map images.\n",
    "        labels (np.ndarray): Corresponding labels (can be one-hot encoded).\n",
    "        title (str): Title for the visualization.\n",
    "        classes (list, optional): Class names corresponding to labels.\n",
    "        cmap (str): Colormap for grayscale images.\n",
    "    \"\"\"\n",
    "    # Convert one-hot labels to scalar indices if necessary\n",
    "    if len(labels.shape) > 1:  # Check if labels are one-hot encoded\n",
    "        labels = np.argmax(labels, axis=1)\n",
    "\n",
    "    # Plot class distribution\n",
    "    class_counts = Counter(labels)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(class_counts.keys(), class_counts.values())\n",
    "    plt.title(f\"Class Distribution: {title}\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    if classes:\n",
    "        plt.xticks(range(len(classes)), classes, rotation=45)\n",
    "    else:\n",
    "        plt.xticks(range(max(class_counts.keys()) + 1))\n",
    "    plt.show()\n",
    "\n",
    "    # Display random samples from the dataset\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    indices = np.random.choice(len(data), size=5, replace=False)\n",
    "    for i, idx in enumerate(indices):\n",
    "        axes[i].imshow(data[idx].squeeze(), cmap=cmap)\n",
    "        axes[i].set_title(f\"Class: {labels[idx]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(f\"Random Samples: {title}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset1 (WM811K)\n",
    "wm811k_data, wm811k_labels = preprocess_wm811k_data(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\")\n",
    "\n",
    "# Class Names for WM811K\n",
    "wm811k_classes = ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Random', 'Scratch', 'Near-full', 'None']\n",
    "\n",
    "# Visualize WM811K Dataset\n",
    "visualize_dataset(wm811k_data, wm811k_labels, title=\"WM811K Dataset (Single Defect)\", classes=wm811k_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset2 (MixedWM38)\n",
    "mixed_data, mixed_labels = preprocess_npz_dataset(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/Wafer_Map_Datasets.npz\")\n",
    "\n",
    "# Class Names for MixedWM38\n",
    "mixed_classes = ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Random', 'Scratch', 'Near-full','None']\n",
    "\n",
    "# Visualize MixedWM38 Dataset\n",
    "visualize_dataset(mixed_data, mixed_labels, title=\"MixedWM38 Dataset (Single + Mixed Defects)\", classes=mixed_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mixed_defects(data, labels, title=\"Mixed Defect Wafer Maps Visualization\", classes=None, cmap='gray'):\n",
    "    \"\"\"\n",
    "    Visualizes the class distribution and random samples for mixed defect wafer maps.\n",
    "\n",
    "    Parameters:\n",
    "        data (np.ndarray): Wafer map images.\n",
    "        labels (np.ndarray): Corresponding labels (one-hot encoded).\n",
    "        title (str): Title for the visualization.\n",
    "        classes (list, optional): Class names corresponding to labels.\n",
    "        cmap (str): Colormap for grayscale images.\n",
    "    \"\"\"\n",
    "    # Identify mixed defect wafer maps (labels with more than one \"1\")\n",
    "    mixed_indices = [i for i, label in enumerate(labels) if np.sum(label) > 1]\n",
    "    mixed_data = data[mixed_indices]\n",
    "    mixed_labels = labels[mixed_indices]\n",
    "\n",
    "    # Convert one-hot labels to scalar indices for mixed defects\n",
    "    scalar_labels = [tuple(np.where(label == 1)[0]) for label in mixed_labels]\n",
    "\n",
    "    # Plot class distribution for mixed defects\n",
    "    label_counts = Counter(scalar_labels)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar([str(k) for k in label_counts.keys()], label_counts.values())\n",
    "    plt.title(f\"Class Distribution: {title}\")\n",
    "    plt.xlabel(\"Mixed Defect Combination\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Display random samples of mixed defects\n",
    "    fig, axes = plt.subplots(1, min(5, len(mixed_data)), figsize=(15, 5))\n",
    "    indices = np.random.choice(len(mixed_data), size=min(5, len(mixed_data)), replace=False)\n",
    "    for i, idx in enumerate(indices):\n",
    "        axes[i].imshow(mixed_data[idx].squeeze(), cmap=cmap)\n",
    "        axes[i].set_title(f\"Defects: {scalar_labels[idx]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(f\"Random Mixed Defect Samples: {title}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset2 (MixedWM38)\n",
    "mixed_data, mixed_labels = preprocess_npz_dataset(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/Wafer_Map_Datasets.npz\")\n",
    "\n",
    "# Class Names for MixedWM38\n",
    "mixed_classes = ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Random', 'Scratch', 'Near-full', 'None']\n",
    "\n",
    "# Visualize Mixed Defect Wafer Maps\n",
    "visualize_mixed_defects(mixed_data, mixed_labels, title=\"MixedWM38 Dataset - Mixed Defects\", classes=mixed_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OG CODE DOWN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from __future__ import print_function\n",
    "import os\n",
    "# import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Preprocessing and Loading the WM811K Dataset\n",
    "def load_wm811k_data(path_to_dataset):\n",
    "    \"\"\"\n",
    "    Load and preprocess the WM811K dataset.\n",
    "\n",
    "    Parameters:\n",
    "        path_to_dataset (str): Path to the WM811K .pkl dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Preprocessed data and labels.\n",
    "    \"\"\"\n",
    "    # Load dataset and drop unnecessary columns\n",
    "    df = pd.read_pickle(path_to_dataset)\n",
    "    df = df.drop(['waferIndex'], axis=1)\n",
    "    df['waferMapDim'] = df['waferMap'].apply(lambda x: (np.size(x, axis=0), np.size(x, axis=1)))\n",
    "    df['failureNum'] = df['failureType']\n",
    "\n",
    "    # Map failure types to numerical values\n",
    "    mapping_type = {\n",
    "        'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3,\n",
    "        'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8\n",
    "    }\n",
    "    df = df.replace({'failureNum': mapping_type})\n",
    "\n",
    "    # Select wafers with valid patterns\n",
    "    df_withpattern = df[(df['failureNum'] >= 0)].reset_index()\n",
    "\n",
    "    wafer_maps = df_withpattern['waferMap'].to_numpy()\n",
    "    labels = df_withpattern['failureNum'].to_numpy()\n",
    "\n",
    "    # Resize images to (32, 32) while preserving aspect ratio and normalize\n",
    "    data = []\n",
    "    for wafer_map in wafer_maps:\n",
    "        # Convert to float32 if not already\n",
    "        image = np.array(wafer_map, dtype=np.float32)\n",
    "\n",
    "        # Apply morphological closing\n",
    "        #kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,1))  # You can adjust the kernel size as needed\n",
    "        #closed_image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "\n",
    "        # Get original dimensions\n",
    "        original_height, original_width = image.shape\n",
    "\n",
    "        # Calculate scaling factor to fit within 32x32\n",
    "        scaling_factor = min(32 / original_width, 32 / original_height)\n",
    "        new_width = int(original_width * scaling_factor)\n",
    "        new_height = int(original_height * scaling_factor)\n",
    "\n",
    "        # Resize image to new dimensions\n",
    "        resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)  #Use cv2.INTER_CUBIC for upscaling and cv2.INTER_AREA for downscaling to preserve image quality:\n",
    "\n",
    "        # Create a blank 32x32 canvas and center the resized image\n",
    "        padded_image = np.full((32, 32), 0, dtype=np.float32)  # Black background\n",
    "        x_offset = (32 - new_width) // 2\n",
    "        y_offset = (32 - new_height) // 2\n",
    "        padded_image[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_image\n",
    "\n",
    "        # Append the padded and resized image\n",
    "        data.append(padded_image)\n",
    "\n",
    "    data = np.expand_dims(data, axis=1)  # Adding channel dimension for PyTorch\n",
    "    data = np.array(data)\n",
    "\n",
    "    # Oversampling to balance the dataset\n",
    "    combined = [(x, y) for x, y in zip(data, labels)]\n",
    "    class_samples = {cls: [sample for sample in combined if sample[1] == cls] for cls in np.unique(labels)}\n",
    "    max_class_size = max(len(samples) for samples in class_samples.values())\n",
    "    \n",
    "    oversampled_samples = []\n",
    "    for cls, samples in class_samples.items():\n",
    "        oversampled_samples.extend(\n",
    "            resample(samples, replace=True, n_samples=max_class_size, random_state=42)\n",
    "            )\n",
    "    np.random.shuffle(oversampled_samples)\n",
    "\n",
    "    # Split back into data and labels after oversampling\n",
    "    data = np.array([sample[0] for sample in oversampled_samples])\n",
    "    labels= np.array([sample[1] for sample in oversampled_samples])\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Custom Dataset for PyTorch\n",
    "class WaferMapDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for wafer maps.\n",
    "\n",
    "        Parameters:\n",
    "            data (np.ndarray): Preprocessed wafer map images.\n",
    "            labels (np.ndarray): Corresponding labels.\n",
    "            transform (callable, optional): Transformations for data augmentation.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def visualize_dataset_split(data, labels, title=\"Dataset Split\"):\n",
    "    \"\"\"\n",
    "    Visualize the class distribution and random samples from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (np.ndarray): Data samples.\n",
    "        labels (np.ndarray): Corresponding labels.\n",
    "        title (str): Title for the visualization.\n",
    "    \"\"\"\n",
    "    # Plot class distribution\n",
    "    class_counts = Counter(labels)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(class_counts.keys(), class_counts.values())\n",
    "    plt.title(f\"Class Distribution: {title}\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(range(max(class_counts.keys()) + 1))\n",
    "    plt.show()\n",
    "\n",
    "    # Display a few random samples\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    indices = np.random.choice(len(data), size=5, replace=False)\n",
    "    for i, idx in enumerate(indices):\n",
    "        axes[i].imshow(data[idx].squeeze(), cmap='gray')\n",
    "        axes[i].set_title(f\"Class: {labels[idx]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(f\"Random Samples: {title}\")\n",
    "    plt.show()\n",
    "\n",
    "# Path to WM811K dataset\n",
    "path_to_dataset = \"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\"\n",
    "\n",
    "# Load and preprocess the data\n",
    "data, labels = load_wm811k_data(path_to_dataset)\n",
    "\n",
    "# Define data transformations (if needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization for grayscale images\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = WaferMapDataset(data, labels, transform=None)  # Apply transforms if needed\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example: Check a batch of data\n",
    "for batch_images, batch_labels in dataloader:\n",
    "    print(\"Batch images shape:\", batch_images.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Classifier(nn.Module):\n",
    "    def __init__(self, init_weights=True, num_class=10,  feature_dim = 512):\n",
    "        self.num_class = num_class\n",
    "        super(VGG16Classifier, self).__init__()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(feature_dim, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_class),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        loss = 0\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid for multi-label classification\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "                \n",
    "def make_variable(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.to('cuda')\n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "def load_training(root_path, dir, batch_size, kwargs):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    data = datasets.ImageFolder(root=root_path + dir, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "    return train_loader\n",
    "\n",
    "def load_testing(root_path, dir, batch_size, kwargs):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    data = datasets.ImageFolder(root=root_path + dir, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    return test_loader\n",
    "\n",
    "def load_datasets(root_path, src_dataset, tgt_dataset, batch_size):\n",
    "    no_cuda = False\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"\n",
    "    cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(8)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(8)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "    src_data_loader = load_training(root_path, src_dataset, batch_size, kwargs)\n",
    "    tgt_data_loader = load_training(root_path, tgt_dataset, batch_size, kwargs)\n",
    "    tgt_data_test = load_testing(root_path, tgt_dataset, batch_size, kwargs)\n",
    "    return src_data_loader, tgt_data_loader, tgt_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDADA(object):\n",
    "\n",
    "    def __init__(self, pkl_file_path, num_class=8, max_epoch=120, batch_size=32, learning_rate=0.0001 ):\n",
    "        self.pkl_file_path = pkl_file_path\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.t_correct = 0\n",
    "        self.updata = 6\n",
    "\n",
    "        # Load the dataset from the .pkl file\n",
    "        data, labels = load_wm811k_data(self.pkl_file_path)\n",
    "\n",
    "        # Split into training and testing datasets\n",
    "        split_ratio = 0.8\n",
    "        split_index = int(len(data) * split_ratio)\n",
    "        src_data, tgt_data = data[:split_index], data[split_index:]\n",
    "        src_labels, tgt_labels = labels[:split_index], labels[split_index:]\n",
    "\n",
    "        # Visualize training and testing datasets\n",
    "        visualize_dataset_split(src_data, src_labels, title=\"Training Dataset\")\n",
    "        visualize_dataset_split(tgt_data, tgt_labels, title=\"Testing Dataset\")\n",
    "\n",
    "        self.datasets_source = DataLoader(\n",
    "            WaferMapDataset(src_data, src_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        self.dataset_target = DataLoader(\n",
    "            WaferMapDataset(tgt_data, tgt_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        self.dataset_target_test = DataLoader(\n",
    "            WaferMapDataset(tgt_data, tgt_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Load VGG16 model\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        model.load_state_dict(torch.load('C:/Users/Soumya Taneja/Desktop/Sideproject/vgg16-397923af.pth'))\n",
    "\n",
    "        pretrained_weights = model.features[0].weight.mean(dim=1, keepdim=True)\n",
    "        model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        model.features[0].weight.data = pretrained_weights\n",
    "        \n",
    "        for i, para in enumerate(model.features.parameters()):\n",
    "            if i < 24:\n",
    "                para.requires_grad = False\n",
    "\n",
    "        \n",
    "        self.Generator = model.features\n",
    "        self.Generator.to('cuda')  # Move the model to GPU\n",
    "\n",
    "        # Debugging: Check the shape of the output from Generator\n",
    "        sample_input = torch.randn(1, 1, 32, 32).to('cuda')  # Example input (batch size 1, 1 channel, 32x32)\n",
    "        sample_output = self.Generator(sample_input)\n",
    "        #print(\"Shape of Generator output:\", sample_output.shape)\n",
    "        # Determine feature_dim dynamically\n",
    "        feature_dim = 512\n",
    "        self.Classifier = VGG16Classifier(num_class=num_class, feature_dim =feature_dim)\n",
    "        self.Classifier1 = VGG16Classifier(num_class=num_class, feature_dim =feature_dim)\n",
    "        self.Classifier2 = VGG16Classifier(num_class=num_class, feature_dim =feature_dim)\n",
    "       \n",
    "        self.Generator.to('cuda')\n",
    "        self.Classifier.to('cuda')\n",
    "        self.Classifier1.to('cuda')\n",
    "        self.Classifier2.to('cuda')\n",
    "\n",
    "        self.opt_generator = optim.Adam(filter(lambda p: p.requires_grad, self.Generator.parameters()),\n",
    "                                        lr=self.lr, weight_decay=0.0005)\n",
    "        self.opt_classifier = optim.Adam(self.Classifier.parameters(),\n",
    "                                         lr=self.lr, weight_decay=0.0005)\n",
    "        self.opt_classifier1 = optim.Adam(self.Classifier1.parameters(),\n",
    "                                          lr=self.lr, weight_decay=0.0005)\n",
    "        self.opt_classifier2 = optim.Adam(self.Classifier2.parameters(),\n",
    "                                          lr=self.lr, weight_decay=0.0005)\n",
    "        \n",
    "        # For tracking learning curve\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.opt_generator.zero_grad()\n",
    "        self.opt_classifier.zero_grad()\n",
    "        self.opt_classifier1.zero_grad()\n",
    "        self.opt_classifier2.zero_grad()\n",
    "\n",
    "    def test(self):\n",
    "        self.Generator.eval()\n",
    "        self.Classifier.eval()\n",
    "        correct = 0\n",
    "        size = 0\n",
    "        val_loss = 0 \n",
    "\n",
    "        criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for data, target in self.dataset_target_test:\n",
    "                img = make_variable(data).to('cuda')\n",
    "                label = make_variable(target).to('cuda')\n",
    "\n",
    "                feat = self.Generator(img)\n",
    "                pred = self.Classifier(feat)\n",
    "\n",
    "                loss = criterion(pred, label)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                pred = pred.argmax(dim=1)  #pred = pred.data.max(1)[1]\n",
    "                k = label.data.size()[0]\n",
    "                correct += pred.eq(label).sum().item()\n",
    "                size += k\n",
    "\n",
    "        val_accuracy = 100.0 * correct / size\n",
    "        val_loss = val_loss / len(self.dataset_target_test)\n",
    "\n",
    "        # Track validation loss and accuracy\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if correct > self.t_correct:\n",
    "            self.t_correct = correct\n",
    "\n",
    "        print('Accuracy: {}/{} ({:.2f}%) Max Accuracy: {}/{} ({:.2f}%) \\n'.\n",
    "              format(correct, size, 100. * correct / size, self.t_correct, size, 100. * self.t_correct / size))\n",
    "        \n",
    "    def train(self):\n",
    "    \n",
    "        criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "        self.Generator.train()\n",
    "        self.Classifier.train()\n",
    "        self.Classifier1.train()\n",
    "        self.Classifier2.train()\n",
    "        torch.cuda.manual_seed(1)\n",
    "\n",
    "        for ep in range(self.max_epoch):\n",
    "\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            data_zip = enumerate(zip(self.datasets_source, self.dataset_target))\n",
    "            for step, ((images_src, label), (images_tgt, _)) in data_zip:\n",
    "                img_src = make_variable(images_src).to('cuda')\n",
    "                label_src = make_variable(label.squeeze_()).to('cuda')\n",
    "                img_tgt = make_variable(images_tgt).to('cuda')\n",
    "\n",
    "                self.reset_grad()\n",
    "                feat_src = self.Generator(img_src)\n",
    "                pred_src_c = self.Classifier(feat_src)\n",
    "                pred_src_c1 = self.Classifier1(feat_src)\n",
    "                pred_src_c2 = self.Classifier2(feat_src)\n",
    "\n",
    "                loss_src_c = criterion(pred_src_c, label_src)\n",
    "                loss_src_c1 = criterion(pred_src_c1, label_src)\n",
    "                loss_src_c2 = criterion(pred_src_c2, label_src)\n",
    "                loss_src = loss_src_c + loss_src_c1 + loss_src_c2\n",
    "\n",
    "                running_loss += loss_src.item()\n",
    "                _, predicted = pred_src_c.max(1)\n",
    "                correct += predicted.eq(label_src).sum().item()\n",
    "                total += label_src.size(0)\n",
    "\n",
    "                loss_src.backward()\n",
    "                self.opt_generator.step()\n",
    "                self.opt_classifier.step()\n",
    "                self.opt_classifier1.step()\n",
    "                self.opt_classifier2.step()\n",
    "\n",
    "                self.reset_grad()\n",
    "                feat_tgt = self.Generator(img_tgt)\n",
    "                pred_tgt_c1 = self.Classifier1(feat_tgt)\n",
    "                pred_tgt_c2 = self.Classifier2(feat_tgt)\n",
    "                p1 = F.softmax(pred_tgt_c1, dim=1)\n",
    "                p2 = F.softmax(pred_tgt_c2, dim=1)\n",
    "                loss_adv = torch.mean(torch.abs(p1 - p2))\n",
    "                loss_adv.backward()\n",
    "                self.opt_generator.step()\n",
    "\n",
    "            train_loss = running_loss / len(self.datasets_source)\n",
    "            train_accuracy = 100.0 * correct / total\n",
    "\n",
    "            # Track training loss and accuracy\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_accuracy)\n",
    "                \n",
    "\n",
    "            print('Train Epoch:{} Adversarial Loss: {:.6f}'.format(ep+1, loss_adv.item())) #The adversarial loss is the mean absolute difference between the two classifiers' probability outputs.\n",
    "            self.test()\n",
    "                \n",
    "    def plot_learning_curve(self):\n",
    "        epochs = range(1, self.max_epoch + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss curve\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.train_losses, label='Training Loss')\n",
    "        plt.plot(epochs, self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Learning Curve: Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Accuracy curve\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.train_accuracies, label='Training Accuracy')\n",
    "        plt.plot(epochs, self.val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Learning Curve: Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "'''A high discrepancy (large loss_adv) indicates that the two classifiers disagree on the predictions, suggesting poor alignment between the source and target domains.\n",
    "A low discrepancy (small loss_adv) means better alignment between the source and target domains.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdada_model = CDADA(pkl_file_path='C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl', num_class=8)\n",
    "cdada_model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdada_model.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'generator_state_dict': self.Generator.state_dict(),\n",
    "    'classifier_state_dict': self.Classifier.state_dict(),\n",
    "    'classifier1_state_dict': self.Classifier1.state_dict(),\n",
    "    'classifier2_state_dict': self.Classifier2.state_dict()\n",
    "}, 'cdada_model.pth')\n",
    "print(\"Model saved as 'cdada_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "cdada_model = CDADA(pkl_file_path='new_dataset.pkl', num_class=8)\n",
    "\n",
    "# Load the saved model parameters\n",
    "checkpoint = torch.load('cdada_model.pth')\n",
    "cdada_model.Generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "cdada_model.Classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
    "cdada_model.Classifier1.load_state_dict(checkpoint['classifier1_state_dict'])\n",
    "cdada_model.Classifier2.load_state_dict(checkpoint['classifier2_state_dict'])\n",
    "\n",
    "print(\"Model parameters loaded successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
