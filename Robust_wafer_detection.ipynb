{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "CUDA Version Used by PyTorch: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version Used by PyTorch:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from torchvision import models\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Dataset 1 (WM811K)\n",
    "def preprocess_wm811k_data(path_to_dataset, target_size=(32, 32), num_classes =38, multi_label_ratio = 0.3):\n",
    "    \"\"\"\n",
    "    Load and preprocess the WM811K dataset and generate synthetic multi-label data.\n",
    "\n",
    "    Parameters:\n",
    "        path_to_dataset (str): Path to the WM811K .pkl dataset.\n",
    "        target_size (tuple): Desired image size.\n",
    "        num_classes (int): Total number of classes.\n",
    "        multi_label_ratio (float): Fraction of multi-label samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed wafer maps.\n",
    "        np.ndarray: Multi-label one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    df = pd.read_pickle(path_to_dataset)\n",
    "    df = df.drop(['waferIndex'], axis=1)\n",
    "    df['waferMapDim'] = df['waferMap'].apply(lambda x: (np.size(x, axis=0), np.size(x, axis=1)))\n",
    "    df['failureNum'] = df['failureType']\n",
    "\n",
    "    mapping_type = {\n",
    "        'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3,\n",
    "        'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8}\n",
    "    df = df.replace({'failureNum': mapping_type})\n",
    "    df_withpattern = df[(df['failureNum'] >= 0)].reset_index()\n",
    "\n",
    "    wafer_maps = df_withpattern['waferMap'].to_numpy()\n",
    "    labels = df_withpattern['failureNum'].to_numpy()\n",
    "\n",
    "    processed_maps = []\n",
    "    for wafer_map in wafer_maps:\n",
    "        wafer_map = np.array(wafer_map, dtype=np.float32)\n",
    "        wafer_map = (wafer_map - np.min(wafer_map)) / (np.max(wafer_map) - np.min(wafer_map))\n",
    "        scaling_factor = min(target_size[1] / wafer_map.shape[1], target_size[0] / wafer_map.shape[0])\n",
    "        new_width = int(wafer_map.shape[1] * scaling_factor)\n",
    "        new_height = int(wafer_map.shape[0] * scaling_factor)\n",
    "        resized_map = cv2.resize(wafer_map, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "        padded_map = np.full(target_size, 0, dtype=np.float32)\n",
    "        x_offset = (target_size[1] - new_width) // 2\n",
    "        y_offset = (target_size[0] - new_height) // 2\n",
    "        padded_map[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_map\n",
    "        processed_maps.append(padded_map)\n",
    "\n",
    "    # One-hot encode labels\n",
    "    encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "    labels = labels.reshape(-1, 1)  # Reshape for one-hot encoding\n",
    "    one_hot_labels = encoder.fit_transform(labels)\n",
    "\n",
    "    # Pad one-hot labels to 38 dimensions\n",
    "    padded_labels = np.zeros((len(one_hot_labels), num_classes))\n",
    "    padded_labels[:, :one_hot_labels.shape[1]] = one_hot_labels\n",
    "\n",
    "    # Generate synthetic multi-label samples directly without oversampling\n",
    "    num_samples = len(processed_maps)\n",
    "    num_multi_label_samples = int(multi_label_ratio * num_samples)\n",
    "    multi_label_maps = []\n",
    "    multi_label_labels = []\n",
    "\n",
    "    for _ in range(num_multi_label_samples):\n",
    "        indices = np.random.choice(len(processed_maps), size=2, replace=False)\n",
    "        combined_map = np.maximum(processed_maps[indices[0]], processed_maps[indices[1]])\n",
    "        combined_label = np.logical_or(one_hot_labels[indices[0]], one_hot_labels[indices[1]]).astype(np.float32)\n",
    "        # Pad synthetic labels to match 38 dimensions\n",
    "        padded_combined_label = np.zeros(num_classes)\n",
    "        padded_combined_label[:combined_label.shape[0]] = combined_label\n",
    "        multi_label_maps.append(combined_map)\n",
    "        multi_label_labels.append(padded_combined_label)\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    # Combine original and synthetic data\n",
    "    all_maps = np.vstack([np.array(processed_maps), np.array(multi_label_maps)])\n",
    "    all_labels = np.vstack([padded_labels, np.array(multi_label_labels)])\n",
    "    print(f\"Original data shape: {np.array(processed_maps).shape}\")\n",
    "    print(f\"Synthetic data shape: {np.array(multi_label_maps).shape}\")\n",
    "    print(f\"Original label shape: {padded_labels.shape}\")\n",
    "    print(f\"Synthetic label shape: {np.array(multi_label_labels).shape}\")\n",
    "    \n",
    "\n",
    "    return np.expand_dims(all_maps, axis=1), all_labels\n",
    "\n",
    "\n",
    "    # Oversampling for balanced dataset\n",
    "    combined = [(x, y) for x, y in zip(processed_maps, labels)]\n",
    "    class_samples = {cls: [sample for sample in combined if sample[1] == cls] for cls in np.unique(labels)}\n",
    "    max_class_size = max(len(samples) for samples in class_samples.values())\n",
    "    oversampled_samples = []\n",
    "    for cls, samples in class_samples.items():\n",
    "        oversampled_samples.extend(resample(samples, replace=True, n_samples=max_class_size, random_state=42))\n",
    "    np.random.shuffle(oversampled_samples)\n",
    "\n",
    "    # Generate synthetic multi-label samples\n",
    "    num_samples = len(oversampled_samples)\n",
    "    num_multi_label_samples = int(multi_label_ratio * num_samples)\n",
    "\n",
    "    multi_label_maps = []\n",
    "    multi_label_labels = []\n",
    "\n",
    "    for _ in range(num_multi_label_samples):\n",
    "        # Randomly select two or more samples to combine\n",
    "        indices = np.random.choice(len(oversampled_samples), size=2, replace=False)\n",
    "        combined_map = np.maximum(oversampled_samples[indices[0]][0], oversampled_samples[indices[1]][0])\n",
    "        combined_label = np.logical_or(oversampled_samples[indices[0]][1], oversampled_samples[indices[1]][1]).astype(np.float32)\n",
    "\n",
    "        multi_label_maps.append(combined_map)\n",
    "        multi_label_labels.append(combined_label)\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    all_maps = np.vstack([np.array([sample[0] for sample in oversampled_samples]), multi_label_maps])\n",
    "    all_labels = np.vstack([np.array([sample[1] for sample in oversampled_samples]), multi_label_labels])\n",
    "\n",
    "    return np.expand_dims(all_maps, axis=1), all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Dataset 2 (MixedWM38)\n",
    "def preprocess_npz_dataset(path, target_size=(32, 32), num_classes=38):\n",
    "    \"\"\"\n",
    "    Preprocess the .npz dataset for mixed-type wafer maps.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the .npz dataset.\n",
    "        target_size (tuple): Desired image size.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed wafer maps.\n",
    "        np.ndarray: Multi-label one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    data = np.load(path)\n",
    "    wafer_maps = data['arr_0']\n",
    "    labels = data['arr_1']\n",
    "\n",
    "    \n",
    "\n",
    "    processed_maps = []\n",
    "    for wafer_map in wafer_maps:\n",
    "        wafer_map = np.array(wafer_map, dtype=np.float32)\n",
    "        wafer_map = (wafer_map - np.min(wafer_map)) / (np.max(wafer_map) - np.min(wafer_map))\n",
    "        resized_map = cv2.resize(wafer_map, target_size, interpolation=cv2.INTER_AREA)\n",
    "        processed_maps.append(resized_map)\n",
    "\n",
    "    # Pad labels to match the model's expected number of classes\n",
    "    padded_labels = np.zeros((labels.shape[0], num_classes))\n",
    "    padded_labels[:, :labels.shape[1]] = labels\n",
    "\n",
    "    processed_maps = np.expand_dims(processed_maps, axis=1)\n",
    "    return np.array(processed_maps), padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for PyTorch\n",
    "class WaferDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Classifier(nn.Module):\n",
    "    def __init__(self, init_weights=True, num_classes=38, feature_dim=512):\n",
    "        \"\"\"\n",
    "        VGG16-based Classifier for multi-label classification.\n",
    "\n",
    "        Parameters:\n",
    "            init_weights (bool): Whether to initialize weights.\n",
    "            num_class (int): Number of output classes.\n",
    "            feature_dim (int): Feature dimensionality from the feature extractor.\n",
    "        \"\"\"\n",
    "        super(VGG16Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(feature_dim, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten features for linear layers\n",
    "        x = self.classifier(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid for multi-label classification\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the classifier layers.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDADA:\n",
    "    def __init__(self, train_data, train_labels,test_data,test_labels,*, num_classes=38, max_epoch=100, batch_size=32, learning_rate=0.0001, subset_size=None):\n",
    "        \"\"\"\n",
    "        Initializes the CDADA training and evaluation pipeline.\n",
    "\n",
    "        Parameters:\n",
    "            train_data (np.ndarray): Training data.\n",
    "            train_labels (np.ndarray): Labels for the training data.\n",
    "            test_data (np.ndarray): Testing data.\n",
    "            test_labels (np.ndarray): Labels for the testing data.\n",
    "            num_classes (int): Number of classes for multi-label classification.\n",
    "            max_epoch (int): Maximum number of epochs.\n",
    "            batch_size (int): Batch size for training and testing.\n",
    "            learning_rate (float): Learning rate for optimizers.\n",
    "        \"\"\"\n",
    "        # Create subset if subset_size is specified\n",
    "        if subset_size is not None:\n",
    "                    # Generate random shuffled indices for the dataset\n",
    "            total_indices = np.arange(len(train_data))\n",
    "            np.random.shuffle(total_indices)\n",
    "            \n",
    "            # Select the first subset_size indices\n",
    "            subset_indices = total_indices[:subset_size]\n",
    "            \n",
    "            # Create the subset dataset using the shuffled indices\n",
    "            subset_data = train_data[subset_indices]\n",
    "            subset_labels = train_labels[subset_indices]\n",
    "            self.train_loader = DataLoader(WaferDataset(subset_data, subset_labels), batch_size=batch_size, shuffle=True)\n",
    "        else:\n",
    "            # Use full dataset\n",
    "            self.train_loader = DataLoader(WaferDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
    "        #self.train_loader = DataLoader(WaferDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(WaferDataset(test_data, test_labels), batch_size=batch_size, shuffle=False)\n",
    "        #self.pkl_file_path = pkl_file_path\n",
    "        #self.npz_file_path = npz_file_path\n",
    "        self.num_classes = num_classes\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "\n",
    "        # Initialize VGG16 model\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  # Modify for grayscale input\n",
    "        self.feature_extractor = model.features.to(device)\n",
    "        self.classifier = VGG16Classifier(num_classes=num_classes).to(device)\n",
    "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # Initialize classifiers\n",
    "        #feature_dim = 512\n",
    "    \n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.opt_generator.zero_grad()\n",
    "        self.opt_classifier.zero_grad()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model on the source dataset.\n",
    "        \"\"\"\n",
    "        self.feature_extractor.train()\n",
    "        self.classifier.train()\n",
    "       \n",
    "\n",
    "        for ep in range(self.max_epoch):\n",
    "            running_loss = 0\n",
    "            for images, labels in self.train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                features = self.feature_extractor(images)\n",
    "                outputs = self.classifier(features)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "        \n",
    "            print(f\"Epoch {ep + 1}/{self.max_epoch}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "            \n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Tests the model on the MixedWM38 dataset.\n",
    "        \"\"\"\n",
    "        self.feature_extractor.eval()\n",
    "        self.classifier.eval()\n",
    "\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "\n",
    "                features = self.feature_extractor(images)\n",
    "                outputs = self.classifier(features)\n",
    "                # all_labels.append(labels.cpu())\n",
    "                # all_outputs.append(outputs.cpu())\n",
    "                all_labels.append(labels.to(device))\n",
    "                all_outputs.append(outputs.to(device))\n",
    "                \n",
    "                # Accuracy calculation\n",
    "                predicted = (outputs > 0.5).float()  # Apply threshold for multi-label classification\n",
    "                correct_predictions += (predicted == labels).all(dim=1).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_outputs = torch.cat(all_outputs)\n",
    "        # Calculate accuracy\n",
    "        accuracy = 100 * correct_predictions / total_samples\n",
    "        precision = precision_score(all_labels, all_outputs.round(), average='samples')\n",
    "        recall = recall_score(all_labels, all_outputs.round(), average='samples')\n",
    "        f1 = f1_score(all_labels, all_outputs.round(), average='samples')\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (172950, 32, 32)\n",
      "Synthetic data shape: (51885, 32, 32)\n",
      "Original label shape: (172950, 38)\n",
      "Synthetic label shape: (51885, 38)\n",
      "Preprocessed data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data, train_labels = preprocess_wm811k_data(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\", multi_label_ratio=0.3)\n",
    "test_data, test_labels = preprocess_npz_dataset(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/Wafer_Map_Datasets.npz\", target_size=(32, 32))\n",
    "\n",
    "# Save train data and labels\n",
    "np.savez_compressed(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/preprocessed_train_data.npz\", data=train_data, labels=train_labels)\n",
    "\n",
    "# Save test data and labels\n",
    "np.savez_compressed(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/preprocessed_test_data.npz\", data=test_data, labels=test_labels)\n",
    "\n",
    "print(\"Preprocessed data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 121.2968\n",
      "Epoch 2/50, Loss: 100.6352\n",
      "Epoch 3/50, Loss: 95.5874\n",
      "Epoch 4/50, Loss: 92.5984\n",
      "Epoch 5/50, Loss: 90.6603\n"
     ]
    }
   ],
   "source": [
    "# Load train data and labels\n",
    "train_data = np.load(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/preprocessed_train_data.npz\")['data']\n",
    "train_labels = np.load(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/preprocessed_train_data.npz\")['labels']\n",
    "\n",
    "# Load test data and labels\n",
    "test_data = np.load(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/preprocessed_test_data.npz\")['data']\n",
    "test_labels = np.load(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/preprocessed_test_data.npz\")['labels']\n",
    "\n",
    "print(\"Preprocessed data loaded successfully!\")\n",
    "cdada = CDADA(train_data, train_labels, test_data, test_labels, num_classes=38, max_epoch=50, subset_size = None)\n",
    "cdada.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdada.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def visualize_dataset(data, labels, title=\"Dataset Visualization\", classes=None, cmap='gray'):\n",
    "    \"\"\"\n",
    "    Visualizes the class distribution and random samples from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (np.ndarray): Wafer map images.\n",
    "        labels (np.ndarray): Corresponding labels (can be one-hot encoded).\n",
    "        title (str): Title for the visualization.\n",
    "        classes (list, optional): Class names corresponding to labels.\n",
    "        cmap (str): Colormap for grayscale images.\n",
    "    \"\"\"\n",
    "    # Convert one-hot labels to scalar indices if necessary\n",
    "    if len(labels.shape) > 1:  # Check if labels are one-hot encoded\n",
    "        labels = np.argmax(labels, axis=1)\n",
    "\n",
    "    # Plot class distribution\n",
    "    class_counts = Counter(labels)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(class_counts.keys(), class_counts.values())\n",
    "    plt.title(f\"Class Distribution: {title}\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    if classes:\n",
    "        plt.xticks(range(len(classes)), classes, rotation=45)\n",
    "    else:\n",
    "        plt.xticks(range(max(class_counts.keys()) + 1))\n",
    "    plt.show()\n",
    "\n",
    "    # Display random samples from the dataset\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    indices = np.random.choice(len(data), size=5, replace=False)\n",
    "    for i, idx in enumerate(indices):\n",
    "        axes[i].imshow(data[idx].squeeze(), cmap=cmap)\n",
    "        axes[i].set_title(f\"Class: {labels[idx]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(f\"Random Samples: {title}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset1 (WM811K)\n",
    "wm811k_data, wm811k_labels = preprocess_wm811k_data(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\")\n",
    "\n",
    "# Class Names for WM811K\n",
    "wm811k_classes = ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Random', 'Scratch', 'Near-full', 'None']\n",
    "\n",
    "# Visualize WM811K Dataset\n",
    "visualize_dataset(wm811k_data, wm811k_labels, title=\"WM811K Dataset (Single Defect)\", classes=wm811k_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset2 (MixedWM38)\n",
    "mixed_data, mixed_labels = preprocess_npz_dataset(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/Wafer_Map_Datasets.npz\")\n",
    "\n",
    "# Class Names for MixedWM38\n",
    "mixed_classes = ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Random', 'Scratch', 'Near-full','None']\n",
    "\n",
    "# Visualize MixedWM38 Dataset\n",
    "visualize_dataset(mixed_data, mixed_labels, title=\"MixedWM38 Dataset (Single + Mixed Defects)\", classes=mixed_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mixed_defects(data, labels, title=\"Mixed Defect Wafer Maps Visualization\", classes=None, cmap='gray'):\n",
    "    \"\"\"\n",
    "    Visualizes the class distribution and random samples for mixed defect wafer maps.\n",
    "\n",
    "    Parameters:\n",
    "        data (np.ndarray): Wafer map images.\n",
    "        labels (np.ndarray): Corresponding labels (one-hot encoded).\n",
    "        title (str): Title for the visualization.\n",
    "        classes (list, optional): Class names corresponding to labels.\n",
    "        cmap (str): Colormap for grayscale images.\n",
    "    \"\"\"\n",
    "    # Identify mixed defect wafer maps (labels with more than one \"1\")\n",
    "    mixed_indices = [i for i, label in enumerate(labels) if np.sum(label) > 1]\n",
    "    mixed_data = data[mixed_indices]\n",
    "    mixed_labels = labels[mixed_indices]\n",
    "\n",
    "    # Convert one-hot labels to scalar indices for mixed defects\n",
    "    scalar_labels = [tuple(np.where(label == 1)[0]) for label in mixed_labels]\n",
    "\n",
    "    # Plot class distribution for mixed defects\n",
    "    label_counts = Counter(scalar_labels)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar([str(k) for k in label_counts.keys()], label_counts.values())\n",
    "    plt.title(f\"Class Distribution: {title}\")\n",
    "    plt.xlabel(\"Mixed Defect Combination\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Display random samples of mixed defects\n",
    "    fig, axes = plt.subplots(1, min(5, len(mixed_data)), figsize=(15, 5))\n",
    "    indices = np.random.choice(len(mixed_data), size=min(5, len(mixed_data)), replace=False)\n",
    "    for i, idx in enumerate(indices):\n",
    "        axes[i].imshow(mixed_data[idx].squeeze(), cmap=cmap)\n",
    "        axes[i].set_title(f\"Defects: {scalar_labels[idx]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(f\"Random Mixed Defect Samples: {title}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset2 (MixedWM38)\n",
    "mixed_data, mixed_labels = preprocess_npz_dataset(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/Wafer_Map_Datasets.npz\")\n",
    "\n",
    "# Class Names for MixedWM38\n",
    "mixed_classes = ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Random', 'Scratch', 'Near-full', 'None']\n",
    "\n",
    "# Visualize Mixed Defect Wafer Maps\n",
    "visualize_mixed_defects(mixed_data, mixed_labels, title=\"MixedWM38 Dataset - Mixed Defects\", classes=mixed_classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
