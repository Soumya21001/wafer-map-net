{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\pandas\\io\\pickle.py:206\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    205\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[1;32m--> 206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.indexes'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\pandas\\io\\pickle.py:211\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# e.g. can occur for files written in py27; see GH#28645 and GH#31988\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\pandas\\compat\\pickle_compat.py:231\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fh, encoding, is_verbose)\u001b[0m\n\u001b[0;32m    229\u001b[0m     up\u001b[38;5;241m.\u001b[39mis_verbose \u001b[38;5;241m=\u001b[39m is_verbose  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m up\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1213\u001b[0m         dispatch[key[\u001b[38;5;241m0\u001b[39m]](\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\pickle.py:1347\u001b[0m, in \u001b[0;36m_Unpickler.load_binstring\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1346\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[1;32m-> 1347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_string(data))\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\pickle.py:1329\u001b[0m, in \u001b[0;36m_Unpickler._decode_string\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0x9a in position 6: ordinal not in range(128)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 133\u001b[0m\n\u001b[0;32m    130\u001b[0m path_to_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Load and preprocess the data\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m load_wm811k_data(path_to_dataset)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Define data transformations (if needed)\u001b[39;00m\n\u001b[0;32m    136\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m    137\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m])  \u001b[38;5;66;03m# Normalization for grayscale images\u001b[39;00m\n\u001b[0;32m    138\u001b[0m ])\n",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m, in \u001b[0;36mload_wm811k_data\u001b[1;34m(path_to_dataset)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03mLoad and preprocess the WM811K dataset.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    tuple: Preprocessed data and labels.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Load dataset and drop unnecessary columns\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(path_to_dataset)\n\u001b[0;32m     33\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwaferIndex\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwaferMapDim\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwaferMap\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (np\u001b[38;5;241m.\u001b[39msize(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), np\u001b[38;5;241m.\u001b[39msize(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\pandas\\io\\pickle.py:214\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# e.g. can occur for files written in py27; see GH#28645 and GH#31988\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\pandas\\compat\\pickle_compat.py:231\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fh, encoding, is_verbose)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# \"Unpickler\" has no attribute \"is_verbose\"  [attr-defined]\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     up\u001b[38;5;241m.\u001b[39mis_verbose \u001b[38;5;241m=\u001b[39m is_verbose  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m up\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\pickle.py:1209\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1209\u001b[0m         key \u001b[38;5;241m=\u001b[39m read(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1210\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n\u001b[0;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\pickle.py:298\u001b[0m, in \u001b[0;36m_Unframer.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_read(n)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from __future__ import print_function\n",
    "import os\n",
    "# import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Preprocessing and Loading the WM811K Dataset\n",
    "def load_wm811k_data(path_to_dataset):\n",
    "    \"\"\"\n",
    "    Load and preprocess the WM811K dataset.\n",
    "\n",
    "    Parameters:\n",
    "        path_to_dataset (str): Path to the WM811K .pkl dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Preprocessed data and labels.\n",
    "    \"\"\"\n",
    "    # Load dataset and drop unnecessary columns\n",
    "    df = pd.read_pickle(path_to_dataset)\n",
    "    df = df.drop(['waferIndex'], axis=1)\n",
    "    df['waferMapDim'] = df['waferMap'].apply(lambda x: (np.size(x, axis=0), np.size(x, axis=1)))\n",
    "    df['failureNum'] = df['failureType']\n",
    "\n",
    "    # Map failure types to numerical values\n",
    "    mapping_type = {\n",
    "        'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3,\n",
    "        'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8\n",
    "    }\n",
    "    df = df.replace({'failureNum': mapping_type})\n",
    "\n",
    "    # Select wafers with valid patterns\n",
    "    df_withpattern = df[(df['failureNum'] >= 0) & (df['failureNum'] <= 7)].reset_index()\n",
    "\n",
    "    wafer_maps = df_withpattern['waferMap'].to_numpy()\n",
    "    labels = df_withpattern['failureNum'].to_numpy()\n",
    "\n",
    "    # Resize images to (32, 32) while preserving aspect ratio and normalize\n",
    "    data = []\n",
    "    for wafer_map in wafer_maps:\n",
    "        # Convert to float32 if not already\n",
    "        image = np.array(wafer_map, dtype=np.float32)\n",
    "\n",
    "        # Apply morphological closing\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (4,4))  # You can adjust the kernel size as needed\n",
    "        closed_image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "\n",
    "        # Get original dimensions\n",
    "        original_height, original_width = closed_image.shape\n",
    "\n",
    "        # Calculate scaling factor to fit within 32x32\n",
    "        scaling_factor = min(32 / original_width, 32 / original_height)\n",
    "        new_width = int(original_width * scaling_factor)\n",
    "        new_height = int(original_height * scaling_factor)\n",
    "\n",
    "        # Resize image to new dimensions\n",
    "        resized_image = cv2.resize(closed_image, (new_width, new_height), interpolation=cv2.INTER_AREA)  #Use cv2.INTER_CUBIC for upscaling and cv2.INTER_AREA for downscaling to preserve image quality:\n",
    "\n",
    "        # Create a blank 32x32 canvas and center the resized image\n",
    "        padded_image = np.full((32, 32), 0, dtype=np.float32)  # Black background\n",
    "        x_offset = (32 - new_width) // 2\n",
    "        y_offset = (32 - new_height) // 2\n",
    "        padded_image[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_image\n",
    "\n",
    "        # Append the padded and resized image\n",
    "        data.append(padded_image)\n",
    "\n",
    "    data = np.expand_dims(data, axis=1)  # Adding channel dimension for PyTorch\n",
    "    data = np.array(data)\n",
    "\n",
    "    # Oversampling to balance the dataset\n",
    "    combined = [(x, y) for x, y in zip(data, labels)]\n",
    "    class_samples = {cls: [sample for sample in combined if sample[1] == cls] for cls in np.unique(labels)}\n",
    "    max_class_size = max(len(samples) for samples in class_samples.values())\n",
    "    \n",
    "    oversampled_samples = []\n",
    "    for cls, samples in class_samples.items():\n",
    "        oversampled_samples.extend(\n",
    "            resample(samples, replace=True, n_samples=max_class_size, random_state=42)\n",
    "            )\n",
    "    np.random.shuffle(oversampled_samples)\n",
    "\n",
    "    # Split back into data and labels after oversampling\n",
    "    data = np.array([sample[0] for sample in oversampled_samples])\n",
    "    labels= np.array([sample[1] for sample in oversampled_samples])\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Custom Dataset for PyTorch\n",
    "class WaferMapDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for wafer maps.\n",
    "\n",
    "        Parameters:\n",
    "            data (np.ndarray): Preprocessed wafer map images.\n",
    "            labels (np.ndarray): Corresponding labels.\n",
    "            transform (callable, optional): Transformations for data augmentation.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Path to WM811K dataset\n",
    "path_to_dataset = \"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\"\n",
    "\n",
    "# Load and preprocess the data\n",
    "data, labels = load_wm811k_data(path_to_dataset)\n",
    "\n",
    "# Define data transformations (if needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization for grayscale images\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = WaferMapDataset(data, labels, transform=None)  # Apply transforms if needed\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example: Check a batch of data\n",
    "for batch_images, batch_labels in dataloader:\n",
    "    print(\"Batch images shape:\", batch_images.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Classifier(nn.Module):\n",
    "    def __init__(self, init_weights=True, num_class=10,  feature_dim = 512):\n",
    "        self.num_class = num_class\n",
    "        super(VGG16Classifier, self).__init__()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(feature_dim, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_class),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        loss = 0\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "                \n",
    "def make_variable(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.to('cuda')\n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "def load_training(root_path, dir, batch_size, kwargs):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    data = datasets.ImageFolder(root=root_path + dir, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "    return train_loader\n",
    "\n",
    "def load_testing(root_path, dir, batch_size, kwargs):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    data = datasets.ImageFolder(root=root_path + dir, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    return test_loader\n",
    "\n",
    "def load_datasets(root_path, src_dataset, tgt_dataset, batch_size):\n",
    "    no_cuda = False\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"\n",
    "    cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(8)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(8)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "    src_data_loader = load_training(root_path, src_dataset, batch_size, kwargs)\n",
    "    tgt_data_loader = load_training(root_path, tgt_dataset, batch_size, kwargs)\n",
    "    tgt_data_test = load_testing(root_path, tgt_dataset, batch_size, kwargs)\n",
    "    return src_data_loader, tgt_data_loader, tgt_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def visualize_dataset_split(data, labels, title=\"Dataset Split\"):\n",
    "    \"\"\"\n",
    "    Visualize the class distribution and random samples from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (np.ndarray): Data samples.\n",
    "        labels (np.ndarray): Corresponding labels.\n",
    "        title (str): Title for the visualization.\n",
    "    \"\"\"\n",
    "    # Plot class distribution\n",
    "    class_counts = Counter(labels)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(class_counts.keys(), class_counts.values())\n",
    "    plt.title(f\"Class Distribution: {title}\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(range(max(class_counts.keys()) + 1))\n",
    "    plt.show()\n",
    "\n",
    "    # Display a few random samples\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    indices = np.random.choice(len(data), size=5, replace=False)\n",
    "    for i, idx in enumerate(indices):\n",
    "        axes[i].imshow(data[idx].squeeze(), cmap='gray')\n",
    "        axes[i].set_title(f\"Class: {labels[idx]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(f\"Random Samples: {title}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDADA(object):\n",
    "\n",
    "    def __init__(self, pkl_file_path, num_class=8, max_epoch=100, batch_size=32, learning_rate=0.0001 ):\n",
    "        self.pkl_file_path = pkl_file_path\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.t_correct = 0\n",
    "        self.updata = 6\n",
    "\n",
    "        # Load the dataset from the .pkl file\n",
    "        data, labels = load_wm811k_data(self.pkl_file_path)\n",
    "\n",
    "        # Split into training and testing datasets\n",
    "        split_ratio = 0.8\n",
    "        split_index = int(len(data) * split_ratio)\n",
    "        src_data, tgt_data = data[:split_index], data[split_index:]\n",
    "        src_labels, tgt_labels = labels[:split_index], labels[split_index:]\n",
    "\n",
    "        # Visualize training and testing datasets\n",
    "        visualize_dataset_split(src_data, src_labels, title=\"Training Dataset\")\n",
    "        visualize_dataset_split(tgt_data, tgt_labels, title=\"Testing Dataset\")\n",
    "\n",
    "        self.datasets_source = DataLoader(\n",
    "            WaferMapDataset(src_data, src_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        self.dataset_target = DataLoader(\n",
    "            WaferMapDataset(tgt_data, tgt_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        self.dataset_target_test = DataLoader(\n",
    "            WaferMapDataset(tgt_data, tgt_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Load VGG16 model\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        model.load_state_dict(torch.load('C:/Users/Soumya Taneja/Desktop/Sideproject/vgg16-397923af.pth'))\n",
    "\n",
    "        pretrained_weights = model.features[0].weight.mean(dim=1, keepdim=True)\n",
    "        model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        model.features[0].weight.data = pretrained_weights\n",
    "        \n",
    "        for i, para in enumerate(model.features.parameters()):\n",
    "            if i < 24:\n",
    "                para.requires_grad = False\n",
    "\n",
    "        \n",
    "        self.Generator = model.features\n",
    "        self.Generator.to('cuda')  # Move the model to GPU\n",
    "\n",
    "        # Debugging: Check the shape of the output from Generator\n",
    "        sample_input = torch.randn(1, 1, 32, 32).to('cuda')  # Example input (batch size 1, 1 channel, 32x32)\n",
    "        sample_output = self.Generator(sample_input)\n",
    "        #print(\"Shape of Generator output:\", sample_output.shape)\n",
    "        # Determine feature_dim dynamically\n",
    "        feature_dim = 512\n",
    "        self.Classifier = VGG16Classifier(num_class=num_class, feature_dim =feature_dim)\n",
    "        self.Classifier1 = VGG16Classifier(num_class=num_class, feature_dim =feature_dim)\n",
    "        self.Classifier2 = VGG16Classifier(num_class=num_class, feature_dim =feature_dim)\n",
    "       \n",
    "        self.Generator.to('cuda')\n",
    "        self.Classifier.to('cuda')\n",
    "        self.Classifier1.to('cuda')\n",
    "        self.Classifier2.to('cuda')\n",
    "\n",
    "        self.opt_generator = optim.Adam(filter(lambda p: p.requires_grad, self.Generator.parameters()),\n",
    "                                        lr=self.lr, weight_decay=0.0005)\n",
    "        self.opt_classifier = optim.Adam(self.Classifier.parameters(),\n",
    "                                         lr=self.lr, weight_decay=0.0005)\n",
    "        self.opt_classifier1 = optim.Adam(self.Classifier1.parameters(),\n",
    "                                          lr=self.lr, weight_decay=0.0005)\n",
    "        self.opt_classifier2 = optim.Adam(self.Classifier2.parameters(),\n",
    "                                          lr=self.lr, weight_decay=0.0005)\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.opt_generator.zero_grad()\n",
    "        self.opt_classifier.zero_grad()\n",
    "        self.opt_classifier1.zero_grad()\n",
    "        self.opt_classifier2.zero_grad()\n",
    "\n",
    "    def test(self):\n",
    "        self.Generator.eval()\n",
    "        self.Classifier.eval()\n",
    "        correct = 0\n",
    "        size = 0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for data, target in self.dataset_target_test:\n",
    "                img = make_variable(data).to('cuda')\n",
    "                label = make_variable(target).to('cuda')\n",
    "\n",
    "                feat = self.Generator(img)\n",
    "                pred = self.Classifier(feat)\n",
    "\n",
    "                pred = pred.argmax(dim=1)  #pred = pred.data.max(1)[1]\n",
    "                k = label.data.size()[0]\n",
    "                correct += pred.eq(label).sum().item()\n",
    "                size += k\n",
    "\n",
    "        if correct > self.t_correct:\n",
    "            self.t_correct = correct\n",
    "\n",
    "        print('Accuracy: {}/{} ({:.2f}%) Max Accuracy: {}/{} ({:.2f}%) \\n'.\n",
    "              format(correct, size, 100. * correct / size, self.t_correct, size, 100. * self.t_correct / size))\n",
    "        \n",
    "    def train(self):\n",
    "    \n",
    "        criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "        self.Generator.train()\n",
    "        self.Classifier.train()\n",
    "        self.Classifier1.train()\n",
    "        self.Classifier2.train()\n",
    "        torch.cuda.manual_seed(1)\n",
    "\n",
    "        for ep in range(self.max_epoch):\n",
    "            data_zip = enumerate(zip(self.datasets_source, self.dataset_target))\n",
    "            for step, ((images_src, label), (images_tgt, _)) in data_zip:\n",
    "                img_src = make_variable(images_src).to('cuda')\n",
    "                label_src = make_variable(label.squeeze_()).to('cuda')\n",
    "                img_tgt = make_variable(images_tgt).to('cuda')\n",
    "\n",
    "                self.reset_grad()\n",
    "                feat_src = self.Generator(img_src)\n",
    "                pred_src_c = self.Classifier(feat_src)\n",
    "                pred_src_c1 = self.Classifier1(feat_src)\n",
    "                pred_src_c2 = self.Classifier2(feat_src)\n",
    "\n",
    "                loss_src_c = criterion(pred_src_c, label_src)\n",
    "                loss_src_c1 = criterion(pred_src_c1, label_src)\n",
    "                loss_src_c2 = criterion(pred_src_c2, label_src)\n",
    "                loss_src = loss_src_c + loss_src_c1 + loss_src_c2\n",
    "\n",
    "                loss_src.backward()\n",
    "                self.opt_generator.step()\n",
    "                self.opt_classifier.step()\n",
    "                self.opt_classifier1.step()\n",
    "                self.opt_classifier2.step()\n",
    "\n",
    "                self.reset_grad()\n",
    "                feat_tgt = self.Generator(img_tgt)\n",
    "                pred_tgt_c1 = self.Classifier1(feat_tgt)\n",
    "                pred_tgt_c2 = self.Classifier2(feat_tgt)\n",
    "                p1 = F.softmax(pred_tgt_c1, dim=1)\n",
    "                p2 = F.softmax(pred_tgt_c2, dim=1)\n",
    "                loss_adv = torch.mean(torch.abs(p1 - p2))\n",
    "                loss_adv.backward()\n",
    "                self.opt_generator.step()\n",
    "                \n",
    "\n",
    "            print('Train Epoch:{} Adversarial Loss: {:.6f}'.format(ep+1, loss_adv.item())) #The adversarial loss is the mean absolute difference between the two classifiers' probability outputs.\n",
    "            self.test()\n",
    "                \n",
    "\n",
    "'''A high discrepancy (large loss_adv) indicates that the two classifiers disagree on the predictions, suggesting poor alignment between the source and target domains.\n",
    "A low discrepancy (small loss_adv) means better alignment between the source and target domains.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdada_model = CDADA(pkl_file_path='C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl', num_class=8)\n",
    "cdada_model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
